{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning tutorial with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This tutorial stars out with introducing affine maps\n",
    "f(x) = Ax + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1104c2e10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.1755 -0.3268 -0.5069\n",
      "-0.6602  0.2260  0.1089\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lin = nn.Linear(5, 3)  # maps from R^5 to R^3, paramters A, b\n",
    "# data is 2x5. A maps from 5 to 3... can we \"map\" data under A?\n",
    "data = autograd.Variable(torch.randn(2, 5))\n",
    "print(lin(data))  # yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.5404 -2.2102\n",
      " 2.1130 -0.0040\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 0.0000  0.0000\n",
      " 2.1130  0.0000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In pytorch, most non-linearities are in torch.functional (we have it imported as F)\n",
    "# Note that non-linearites typically don't have parameters like affine maps do.\n",
    "# That is, they don't have weights that are updated during training.\n",
    "data = autograd.Variable(torch.randn(2, 2))\n",
    "print(data)\n",
    "print(F.relu(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax and probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let x be a vector of real numbers (positive, negative, whatever, there are no constraints). Then the i’th component of Softmax(x) is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*} \\frac{\\text{exp}(x_i)}{\\sum_j{exp(x_j)}} \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be clear that the output is a probability distribution: each element is non-negative and the sum over all components is 1.\n",
    "\n",
    "You could also think of it as just applying an element-wise exponentiation operator to the input to make everything non-negative and then dividing by the normalization constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1.3800\n",
      "-1.3505\n",
      " 0.3455\n",
      " 0.5046\n",
      " 1.8213\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "Variable containing:\n",
      " 0.2948\n",
      " 0.0192\n",
      " 0.1048\n",
      " 0.1228\n",
      " 0.4584\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "-1.2214\n",
      "-3.9519\n",
      "-2.2560\n",
      "-2.0969\n",
      "-0.7801\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Softmax is also in torch.nn.functional\n",
    "data = autograd.Variable(torch.randn(5))\n",
    "print(data)\n",
    "print(F.softmax(data, 0))\n",
    "print(F.softmax(data, 0).sum())  # Sums to 1 because it is a distribution!\n",
    "print(F.log_softmax(data, 0))  # theres also log_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s write an annotated example of a network that takes in a sparse bag-of-words representation and outputs a probability distribution over two labels: “English” and “Spanish”. This model is just logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression bag-of-words classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag-of-words (BOW) vector will look like this:\n",
    "\n",
    "`[Count(hello),Count(world)]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the network output will be:\n",
    "\n",
    "`log Softmax(Ax + b)`\n",
    "\n",
    "In other words, pass the input through an affine map and then do <br>`log Softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n"
     ]
    }
   ],
   "source": [
    "# word_to_ix maps each word in the vocab to a unique integer, which will be its\n",
    "# index into the Bag of Words vectors\n",
    "word_to_ix = {}\n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "print(word_to_ix)\n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BOWClassifier(nn.Module):  # inherit from nn.Module!\n",
    "    \n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BOWClassifier, self).__init__()\n",
    "\n",
    "        # Define the parameters that you will need.  In this case, we need A and b,\n",
    "        # the parameters of the affine mapping.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # Make sure you understand why the input dimension is vocab_size\n",
    "        # and the output is num_labels!\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "        # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n",
    "        # to worry about that here\n",
    "        \n",
    "    def forward(self, bow_vec):\n",
    "        # Pass the input through the linear layer\n",
    "        # then pass that through Log_softmax.\n",
    "        # Many Non-linearities and other functions are in torch.nn.functional\n",
    "        return F.log_softmax(self.linear(bow_vec), 1)\n",
    "\n",
    "\n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])\n",
    "\n",
    "\n",
    "model = BOWClassifier(NUM_LABELS, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.1194  0.0609 -0.1268  0.1274  0.1191  0.1739 -0.1099 -0.0323 -0.0038  0.0286\n",
      " 0.1152 -0.1136 -0.1743  0.1427 -0.0291  0.1103  0.0630 -0.1471  0.0394  0.0471\n",
      "\n",
      "Columns 10 to 19 \n",
      "-0.1488 -0.1392  0.1067 -0.0460  0.0958  0.0112  0.0644  0.0431  0.0713  0.0972\n",
      "-0.1313 -0.0931  0.0669  0.0351 -0.0834 -0.0594  0.1796 -0.0363  0.1106  0.0849\n",
      "\n",
      "Columns 20 to 25 \n",
      "-0.1816  0.0987 -0.1379 -0.1480  0.0119 -0.0334\n",
      "-0.1268 -0.1668  0.1882  0.0102  0.1344  0.0406\n",
      "[torch.FloatTensor of size 2x26]\n",
      "\n",
      "Parameter containing:\n",
      " 0.0631\n",
      " 0.1465\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the model knows its parameters.  The first output below is A, the second is b.\n",
    "# Whenever you assign a component to a class variable in the __init__ function\n",
    "# of a module, which was done with the line\n",
    "# self.linear = nn.Linear(...)\n",
    "# Then through some Python magic from the Pytorch devs, your module\n",
    "# (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.5378 -0.8771\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To run the model, pass in a BoW vector, but wrapped in an autograd.Variable\n",
    "sample = data[0]\n",
    "bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
    "log_probs = model(autograd.Variable(bow_vector))\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model.\n",
    "\n",
    "Note that the input to NLLLoss is a vector of log probabilities, and a target label. It doesn’t compute the log probabilities for us. This is why the last layer of our network is log softmax. The loss function nn.CrossEntropyLoss() is the same as NLLLoss(), except it does the log softmax for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.9297 -0.5020\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-0.6388 -0.7506\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "matrix column corresponding to \"creo\" from test w/o training:\n",
      "Variable containing:\n",
      "-0.1488\n",
      "-0.1313\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "Variable containing:\n",
      "-0.2093 -1.6669\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-2.5330 -0.0828\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "after training:\n",
      "Variable containing:\n",
      " 0.2803\n",
      "-0.5605\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run on test data before training to get a 'before and after' \n",
    "for instance, label in test_data:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)\n",
    "\n",
    "\n",
    "# Print the matrix column corresponding to \"creo\"\n",
    "print('matrix column corresponding to \"creo\" from test w/o training:')\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])\n",
    "\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Usually you pass over the training data several times.\n",
    "# 100 (used here) is much bigger than on a real data set, but real datasets have more than\n",
    "# two instances. Usually, somewhere between 5 and 30 epochs is reasonable.\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Step 2. Make our BOW vector and also wrap the target in a \n",
    "        # Variable as an integer. For example, if the target is SPANISH, then \n",
    "        # wrap the integer 0. The Loss Function then knows that the 0th \n",
    "        # element of the log probabilities is the log probability \n",
    "        # that corresponds to SPANISH\n",
    "        bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "        target = autograd.Variable(make_target(label, label_to_ix))\n",
    "        \n",
    "        # Step 3. Run the forward pass\n",
    "        log_probs = model(bow_vec)\n",
    "        \n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by \n",
    "        # calling optimizer.step()\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "for instance, label in test_data:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)\n",
    "\n",
    "\n",
    "# Index corresponding to Spanish goes up, English goes down!\n",
    "print(\"after training:\")\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.6310\n",
      " 0.5841\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters())[:, word_to_ix[\"Give\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings: Encoding lexical semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word embeddings are a representation of the *semantics* of a word, efficiently encoding semantic information that might be relevant to the task at hand. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10c9ffed0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.6614  0.2669  0.0617  0.6213 -0.4519\n",
      "[torch.FloatTensor of size 1x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.LongTensor([word_to_ix[\"hello\"]])\n",
    "hello_embed = embeds(autograd.Variable(lookup_tensor))\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### an example: N-Gram Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "# print the first 3, just so I can see what they look like\n",
    "print(trigrams[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " 516.7384\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 514.2780\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 511.8333\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 509.4035\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 506.9883\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 504.5865\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 502.1965\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 499.8185\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 497.4510\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 495.0930\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for context, target in trigrams:\n",
    "        \n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in variables)\n",
    "        context_idxs = [word_to_ix[w] for w in context]\n",
    "        context_var = autograd.Variable(torch.LongTensor(context_idxs))\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_var)\n",
    "        \n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a variable)\n",
    "        loss = loss_function(log_probs, autograd.Variable(\n",
    "            torch.LongTensor([word_to_ix[target]])))\n",
    "        \n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.data\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Computing word embeddings: Continuous Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CBOW model is as follows. Given a target word *w$_i$* and an *N* context window on each side, $w_{i−1}$,…,w$_{i−N}$ and w$_{i+1}$,…,w$_{i+N}$, referring to all context words collectively as ***C***, CBOW tries to minimize\n",
    "\n",
    "\\begin{equation*} −\\log p(w_i|C)=−\\log \\text{Softmax}(A(\\sum_{w∈C}q_w)+b) \\end{equation*}\n",
    "\n",
    "is the embedding for word *w*.\n",
    "\n",
    "Implement this model in Pytorch by filling in the class below. Some tips:\n",
    "\n",
    "*    Think about which parameters you need to define.\n",
    "*    Make sure you know what shape each operation expects. Use .view() if you need to reshape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).sum(dim=0).view((1, -1))\n",
    "        out = self.linear(embeds)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "\n",
    "\n",
    "make_context_vector(data[0][0], word_to_ix)  # example\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(vocab_size, EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " 260.6818\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 257.9831\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 255.3309\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 252.7230\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 250.1575\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 247.6327\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 245.1471\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 242.6995\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 240.2887\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 237.9137\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 235.5737\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 233.2677\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 230.9952\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 228.7553\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 226.5475\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 224.3713\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 222.2261\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 220.1114\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 218.0267\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 215.9717\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 213.9458\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 211.9487\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 209.9799\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 208.0390\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 206.1256\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 204.2394\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 202.3799\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 200.5466\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 198.7394\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 196.9577\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 195.2011\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 193.4692\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 191.7616\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 190.0779\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 188.4178\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 186.7807\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 185.1664\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 183.5743\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 182.0042\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 180.4555\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 178.9280\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 177.4212\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 175.9348\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 174.4684\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 173.0216\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 171.5941\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 170.1855\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 168.7955\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 167.4237\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 166.0699\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 164.7336\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 163.4147\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 162.1127\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 160.8275\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 159.5586\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 158.3059\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 157.0690\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 155.8476\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 154.6416\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 153.4507\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 152.2746\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 151.1130\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 149.9657\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 148.8325\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 147.7132\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 146.6075\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 145.5152\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 144.4361\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 143.3699\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 142.3166\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 141.2758\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 140.2473\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 139.2310\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 138.2267\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 137.2341\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 136.2531\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 135.2834\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 134.3250\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 133.3776\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 132.4410\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 131.5151\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 130.5996\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 129.6945\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 128.7994\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 127.9144\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 127.0392\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 126.1736\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 125.3174\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 124.4707\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 123.6330\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 122.8044\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 121.9847\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 121.1737\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 120.3713\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 119.5773\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 118.7916\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 118.0141\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 117.2446\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 116.4829\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 115.7291\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for context, target in data:\n",
    "        \n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in variables)\n",
    "        context_idxs = [word_to_ix[w] for w in context]\n",
    "        context_var = autograd.Variable(torch.LongTensor(context_idxs))\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_var)\n",
    "        \n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a variable)\n",
    "        loss = loss_function(log_probs, autograd.Variable(\n",
    "            torch.LongTensor([word_to_ix[target]])))\n",
    "        \n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.data\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'computer', 'our', 'spells.'] 48 5\n"
     ]
    }
   ],
   "source": [
    "#For sanity check, let's see what our model predicts\n",
    "acc = 0\n",
    "for context, target in data:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in variables)\n",
    "        context_idxs = [word_to_ix[w] for w in context]\n",
    "        context_var = autograd.Variable(torch.LongTensor(context_idxs))\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_var)\n",
    "        _,idx = torch.min(log_probs,-1)\n",
    "print (context, word_to_ix[target], idx.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.6186\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function(log_probs, autograd.Variable(torch.LongTensor([word_to_ix[target]]))).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence model and Long-Short Term Memory Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent neural network is a network that maintains some kind of state. For example, its output could be used as part of the next input, so that information can propogate along as the network passes over the sequence. In the case of an LSTM, for each element in the sequence, there is a corresponding hidden state _h_$_{t}$, which in principle can contain information from arbitrary points earlier in the sequence. We can use the hidden state to predict words in a language model, part-of-speech tags, and a myriad of other things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMs in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you could go through the sequence one at a time, in which case the 1st axis will have size 1 also.\n",
    "\n",
    "Let’s see a quick example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10c9ffed0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(3, 3)  # input dim is 3, output dim is 3\n",
    "inputs = [autograd.Variable(torch.randn(1, 3))\n",
    "          for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)),\n",
    "          autograd.Variable(torch.randn((1, 1, 3))))\n",
    "\n",
    "for i in inputs:\n",
    "    # step through the sequence one element at a time\n",
    "    # after each step, hidden contains the hidden state\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.3600  0.0893  0.0215\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      "\n",
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.3600  0.0893  0.0215\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      " -1.1298  0.4467  0.0254\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.0187  0.1713 -0.2944\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.3521  0.1026 -0.2971\n",
      "\n",
      "(2 ,.,.) = \n",
      " -0.3191  0.0781 -0.1957\n",
      "\n",
      "(3 ,.,.) = \n",
      " -0.1634  0.0941 -0.1637\n",
      "\n",
      "(4 ,.,.) = \n",
      " -0.3368  0.0959 -0.0538\n",
      "[torch.FloatTensor of size 5x1x3]\n",
      "\n",
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.3368  0.0959 -0.0538\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.9825  0.4715 -0.0633\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable(\n",
    "    torch.randn((1, 1, 3))))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.3368  0.0959 -0.0538\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.9825  0.4715 -0.0633\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: An LSTM for Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "\n",
    "training_data = [(\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "                 (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])]\n",
    "\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # The linear layer that maps from hidden state to space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.1989 -0.9630 -1.1497\n",
      "-1.2522 -0.9158 -1.1586\n",
      "-1.2563 -1.0022 -1.0550\n",
      "-1.1518 -1.1443 -1.0065\n",
      "-1.1728 -1.0677 -1.0593\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "Variable containing:\n",
      "-0.1902 -1.8654 -3.9957\n",
      "-4.1051 -0.0263 -4.6590\n",
      "-4.0204 -3.1797 -0.0614\n",
      "-0.0372 -4.3504 -3.7448\n",
      "-4.0387 -0.0348 -4.1001\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "print(tag_scores)\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Variables of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        \n",
    "        # Step 3. Run our forward pass\n",
    "        tag_scores = model(sentence_in)\n",
    "        \n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "# See what the scores are after training\n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "# The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "#  for word i. The predicted tag is the maximum scoring tag.\n",
    "# Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "# since 0 is index of the maximum value of row 1,\n",
    "# 1 is the index of maximum value of row 2, etc.\n",
    "# Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " -0.1902\n",
       " -0.0263\n",
       " -0.0614\n",
       " -0.0372\n",
       " -0.0348\n",
       " [torch.FloatTensor of size 5], Variable containing:\n",
       "  0\n",
       "  1\n",
       "  2\n",
       "  0\n",
       "  1\n",
       " [torch.LongTensor of size 5])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(tag_scores, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
